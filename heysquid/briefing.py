"""
ì¼ì¼ ë¸Œë¦¬í•‘ ìƒì„±ê¸° â€” heysquid

ë§¤ì¼ ì•„ì¹¨ í”„ë¡œì íŠ¸ ìƒíƒœë¥¼ ë¶„ì„í•˜ì—¬ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡.

ë¶„ì„ ëŒ€ìƒ:
- ê° ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì˜ git log (ìµœê·¼ 24h)
- tasks/ ë””ë ‰í† ë¦¬ (ë¯¸ì²˜ë¦¬ ì‘ì—…)
- workspaces/{name}/progress.md
- ë©€í‹°ì†ŒìŠ¤ IT/AI ë‰´ìŠ¤ TOP 5
- ìŠ¤ë ˆë“œ ê¸€ ì´ˆì•ˆ ìë™ ìƒì„±

ì‚¬ìš©ë²•:
    python briefing.py          # ìˆ˜ë™ ì‹¤í–‰
    (launchdë¡œ ë§¤ì¼ 09:00 ìë™ ì‹¤í–‰)
"""

import os
import sys
import json
import re
import subprocess
import time
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from urllib.request import urlopen, Request
from urllib.error import URLError
from dotenv import load_dotenv

# ê²½ë¡œ ì„¤ì •
from .config import PROJECT_ROOT_STR as PROJECT_ROOT, get_env_path

# .env ë¡œë“œ
load_dotenv(get_env_path())

CHAT_ID = os.getenv("TELEGRAM_ALLOWED_USERS", "").split(",")[0].strip()


def get_git_summary(repo_path):
    """
    Git ì €ì¥ì†Œì˜ ìµœê·¼ 24ì‹œê°„ ì»¤ë°‹ ìš”ì•½

    Args:
        repo_path: Git ì €ì¥ì†Œ ê²½ë¡œ

    Returns:
        str: ì»¤ë°‹ ìš”ì•½ í…ìŠ¤íŠ¸
    """
    if not os.path.exists(os.path.join(repo_path, ".git")):
        return "(git ì €ì¥ì†Œ ì•„ë‹˜)"

    try:
        since = (datetime.now() - timedelta(hours=24)).strftime("%Y-%m-%d %H:%M:%S")
        result = subprocess.run(
            ["git", "log", "--oneline", f"--since={since}", "--no-merges"],
            cwd=repo_path,
            capture_output=True,
            text=True,
            timeout=10
        )

        if result.returncode != 0:
            return "(git log ì˜¤ë¥˜)"

        lines = result.stdout.strip().split("\n")
        lines = [l for l in lines if l.strip()]

        if not lines:
            return "ìµœê·¼ 24ì‹œê°„ ì»¤ë°‹ ì—†ìŒ"

        return f"ìµœê·¼ 24ì‹œê°„ ì»¤ë°‹ {len(lines)}ê±´:\n" + "\n".join(f"  - {l}" for l in lines[:10])

    except subprocess.TimeoutExpired:
        return "(git log íƒ€ì„ì•„ì›ƒ)"
    except Exception as e:
        return f"(git ì˜¤ë¥˜: {e})"


def get_pending_tasks():
    """ë¯¸ì²˜ë¦¬ í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ìˆ˜ í™•ì¸"""
    messages_file = os.path.join(PROJECT_ROOT, "data", "telegram_messages.json")

    if not os.path.exists(messages_file):
        return 0

    try:
        with open(messages_file, "r", encoding="utf-8") as f:
            data = json.load(f)

        messages = data.get("messages", [])
        pending = [m for m in messages if not m.get("processed", False) and m.get("type") == "user"]
        return len(pending)

    except Exception:
        return 0


def get_recent_progress(name):
    """ìµœê·¼ ì§„í–‰ ê¸°ë¡ (ë§ˆì§€ë§‰ 3ê°œ í•­ëª©)"""
    progress_file = os.path.join(PROJECT_ROOT, "workspaces", name, "progress.md")

    if not os.path.exists(progress_file):
        return "ì§„í–‰ ê¸°ë¡ ì—†ìŒ"

    try:
        with open(progress_file, "r", encoding="utf-8") as f:
            content = f.read()

        # ### ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” í•­ëª© ì¶”ì¶œ
        entries = content.split("### ")
        entries = [e.strip() for e in entries if e.strip() and not e.startswith("#")]

        if not entries:
            return "ì§„í–‰ ê¸°ë¡ ì—†ìŒ"

        # ë§ˆì§€ë§‰ 3ê°œ
        recent = entries[-3:]
        return "\n".join(f"  - {e.split(chr(10))[0]}" for e in recent)

    except Exception:
        return "ì§„í–‰ ê¸°ë¡ ì—†ìŒ"


def fetch_geeknews_top(count=10):
    """GeekNews(news.hada.io) ë©”ì¸ í˜ì´ì§€ì—ì„œ í¬ì¸íŠ¸ ê¸°ì¤€ ì¸ê¸° ë‰´ìŠ¤ ì¶”ì¶œ"""
    try:
        req = Request(
            "https://news.hada.io/",
            headers={"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"}
        )
        with urlopen(req, timeout=15) as resp:
            html = resp.read().decode("utf-8")

        # <h1>ì œëª©</h1> ... topic?id=ID ... id='tpID'>POINTS</span> point
        pattern = r"<h1>(.*?)</h1>.*?topic\?id=(\d+).*?id='tp\d+'>(\d+)</span>\s*point"
        matches = re.findall(pattern, html, re.DOTALL)

        news_items = []
        for title, topic_id, points in matches:
            title = re.sub(r'<[^>]+>', '', title).strip()
            if not title:
                continue
            news_items.append({
                "title": title,
                "url": f"https://news.hada.io/topic?id={topic_id}",
                "points": int(points)
            })

        # í¬ì¸íŠ¸ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        news_items.sort(key=lambda x: x["points"], reverse=True)
        return news_items[:count]

    except (URLError, Exception) as e:
        print(f"[WARN] GeekNews í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        return []


def format_geeknews(news_items):
    """GeekNews ë‰´ìŠ¤ë¥¼ í…”ë ˆê·¸ë¨ í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
    if not news_items:
        return "GeekNews ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆì–´ìš”."

    lines = [f"GeekNews ì˜¤ëŠ˜ì˜ í•« ë‰´ìŠ¤ TOP {len(news_items)}", ""]
    for i, item in enumerate(news_items, 1):
        lines.append(f"{i}. {item['title']}")
        lines.append(f"â­ {item['url']}")
        lines.append("")

    return "\n".join(lines).strip()


# ============================================================
# ë©€í‹°ì†ŒìŠ¤ ë‰´ìŠ¤ ìˆ˜ì§‘ + TOP 5 + ìŠ¤ë ˆë“œ ê¸€ ìƒì„±
# ============================================================

AI_KEYWORDS = re.compile(
    r'\b(ai|a\.i\.|artificial intelligence|llm|gpt|chatgpt|openai|claude|anthropic|'
    r'gemini|copilot|diffusion|stable diffusion|midjourney|machine learning|deep learning|'
    r'neural|transformer|hugging\s?face|langchain|rag|agent|fine.?tun|sora|groq|mistral|'
    r'llama|generative|gen\s?ai)\b',
    re.IGNORECASE
)

_UA = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"


def fetch_hackernews_top(count=30):
    """Hacker News APIì—ì„œ ìƒìœ„ ìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°"""
    try:
        req = Request(
            "https://hacker-news.firebaseio.com/v0/topstories.json",
            headers={"User-Agent": _UA}
        )
        with urlopen(req, timeout=10) as resp:
            story_ids = json.loads(resp.read().decode("utf-8"))

        items = []
        for sid in story_ids[:count]:
            try:
                req = Request(
                    f"https://hacker-news.firebaseio.com/v0/item/{sid}.json",
                    headers={"User-Agent": _UA}
                )
                with urlopen(req, timeout=5) as resp:
                    story = json.loads(resp.read().decode("utf-8"))

                if not story or story.get("type") != "story":
                    continue

                items.append({
                    "title": story.get("title", ""),
                    "url": story.get("url", f"https://news.ycombinator.com/item?id={sid}"),
                    "points": story.get("score", 0),
                    "comments": story.get("descendants", 0),
                    "timestamp": story.get("time", 0),
                    "source": "HackerNews",
                })
            except Exception:
                continue

        return items

    except (URLError, Exception) as e:
        print(f"[WARN] HackerNews API ì˜¤ë¥˜: {e}")
        return []


def _parse_rss(url, source_name, timeout=10):
    """RSS í”¼ë“œë¥¼ íŒŒì‹±í•˜ì—¬ ë‰´ìŠ¤ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    try:
        req = Request(url, headers={"User-Agent": _UA})
        with urlopen(req, timeout=timeout) as resp:
            xml_data = resp.read().decode("utf-8")

        root = ET.fromstring(xml_data)

        items = []
        # RSS 2.0 í˜•ì‹
        for item in root.iter("item"):
            title_el = item.find("title")
            link_el = item.find("link")
            pubdate_el = item.find("pubDate")

            title = title_el.text.strip() if title_el is not None and title_el.text else ""
            link = link_el.text.strip() if link_el is not None and link_el.text else ""

            if not title:
                continue

            ts = 0
            if pubdate_el is not None and pubdate_el.text:
                ts = _parse_rss_date(pubdate_el.text)

            items.append({
                "title": title,
                "url": link,
                "points": 0,
                "comments": 0,
                "timestamp": ts,
                "source": source_name,
            })

        return items

    except (URLError, ET.ParseError, Exception) as e:
        print(f"[WARN] {source_name} RSS ì˜¤ë¥˜: {e}")
        return []


def _parse_rss_date(date_str):
    """RFC 822 ë‚ ì§œ ë¬¸ìì—´ì„ Unix timestampë¡œ ë³€í™˜"""
    # í”í•œ RSS ë‚ ì§œ í˜•ì‹ë“¤
    formats = [
        "%a, %d %b %Y %H:%M:%S %z",
        "%a, %d %b %Y %H:%M:%S %Z",
        "%Y-%m-%dT%H:%M:%S%z",
        "%Y-%m-%dT%H:%M:%SZ",
    ]
    for fmt in formats:
        try:
            dt = datetime.strptime(date_str.strip(), fmt)
            return int(dt.timestamp())
        except ValueError:
            continue
    # fallback: ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ 0
    return 0


def fetch_techcrunch_rss(count=20):
    """TechCrunch RSSì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    items = _parse_rss("https://techcrunch.com/feed/", "TechCrunch")
    return items[:count]


def fetch_mit_tech_review_rss(count=20):
    """MIT Technology Review RSSì—ì„œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    items = _parse_rss(
        "https://www.technologyreview.com/feed/", "MIT Tech Review"
    )
    return items[:count]


def fetch_all_news_sources():
    """ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ì—¬ í†µí•© ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    all_items = []

    # GeekNews (ê¸°ì¡´ í•¨ìˆ˜ í™œìš©, í†µí•© ìŠ¤ì½”ì–´ë§ìš© í•„ë“œ ì¶”ê°€)
    geeknews = fetch_geeknews_top(20)
    for item in geeknews:
        item.setdefault("source", "GeekNews")
        item.setdefault("comments", 0)
        item.setdefault("timestamp", int(time.time()))  # ì‹œê°„ ì •ë³´ ì—†ìœ¼ë¯€ë¡œ í˜„ì¬
        all_items.append(item)

    # Hacker News
    all_items.extend(fetch_hackernews_top(30))

    # TechCrunch
    all_items.extend(fetch_techcrunch_rss(20))

    # MIT Technology Review
    all_items.extend(fetch_mit_tech_review_rss(20))

    print(f"[INFO] ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_items)}ê±´ "
          f"(GN:{len(geeknews)}, HN:{len([i for i in all_items if i['source']=='HackerNews'])}, "
          f"TC:{len([i for i in all_items if i['source']=='TechCrunch'])}, "
          f"MIT:{len([i for i in all_items if i['source']=='MIT Tech Review'])})")

    return all_items


def _title_similarity(a, b):
    """ë‘ ì œëª©ì˜ ë‹¨ì–´ ê²¹ì¹¨ ë¹„ìœ¨ (Jaccard ìœ ì‚¬ë„)"""
    words_a = set(re.findall(r'\w+', a.lower()))
    words_b = set(re.findall(r'\w+', b.lower()))
    if not words_a or not words_b:
        return 0.0
    intersection = words_a & words_b
    union = words_a | words_b
    return len(intersection) / len(union)


def score_and_rank_news(all_items, top_n=5):
    """
    5ê°€ì§€ ê¸°ì¤€ìœ¼ë¡œ ë‰´ìŠ¤ ìŠ¤ì½”ì–´ë§ í›„ ìƒìœ„ Nê°œ ë°˜í™˜.

    ê¸°ì¤€:
    1. ìµœì‹ ì„± (recency): 24ì‹œê°„ ì´ë‚´ ìš°ì„ , 12ì‹œê°„ ì´ë‚´ ë³´ë„ˆìŠ¤
    2. ì†ŒìŠ¤ ë‹¤ì–‘ì„± (cross_source): ì—¬ëŸ¬ ì†ŒìŠ¤ì— ë¹„ìŠ·í•œ ì œëª©ì´ë©´ ê°€ì‚°
    3. ì¸ê¸°ë„ (popularity): points ê¸°ë°˜
    4. AI ê´€ë ¨ì„± (ai_relevance): ì œëª©ì— AI í‚¤ì›Œë“œ í¬í•¨
    5. í† ë¡ ì„± (discussion): comments ìˆ˜ ê¸°ë°˜
    """
    now_ts = time.time()
    max_points = max((item["points"] for item in all_items), default=1) or 1
    max_comments = max((item["comments"] for item in all_items), default=1) or 1

    for item in all_items:
        score = 0.0

        # 1. ìµœì‹ ì„± (0~25ì )
        age_hours = (now_ts - item["timestamp"]) / 3600 if item["timestamp"] > 0 else 48
        if age_hours <= 6:
            score += 25
        elif age_hours <= 12:
            score += 20
        elif age_hours <= 24:
            score += 15
        elif age_hours <= 48:
            score += 5
        # 48ì‹œê°„ ì´ìƒ: 0ì 

        # 2. ì†ŒìŠ¤ ë‹¤ì–‘ì„± (0~20ì ): ë‹¤ë¥¸ ì†ŒìŠ¤ì— ë¹„ìŠ·í•œ ì œëª©ì´ ìˆìœ¼ë©´ ê°€ì‚°
        cross_count = 0
        seen_sources = {item["source"]}
        for other in all_items:
            if other["source"] in seen_sources:
                continue
            if _title_similarity(item["title"], other["title"]) > 0.3:
                cross_count += 1
                seen_sources.add(other["source"])
        score += min(cross_count * 10, 20)

        # 3. ì¸ê¸°ë„ (0~25ì )
        score += (item["points"] / max_points) * 25

        # 4. AI ê´€ë ¨ì„± (0~20ì )
        ai_matches = len(AI_KEYWORDS.findall(item["title"]))
        if ai_matches >= 2:
            score += 20
        elif ai_matches == 1:
            score += 15

        # 5. í† ë¡ ì„± (0~10ì )
        score += (item["comments"] / max_comments) * 10

        item["_score"] = round(score, 2)

    # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    ranked = sorted(all_items, key=lambda x: x["_score"], reverse=True)

    # ì¤‘ë³µ ì œê±° (ìœ ì‚¬ë„ > 0.4ì¸ ê²½ìš° ë‚®ì€ ì ìˆ˜ ê²ƒ ì œê±°)
    deduped = []
    for item in ranked:
        is_dup = False
        for selected in deduped:
            if _title_similarity(item["title"], selected["title"]) > 0.4:
                is_dup = True
                break
        if not is_dup:
            deduped.append(item)
        if len(deduped) >= top_n:
            break

    return deduped


def pick_best_per_criterion(all_items):
    """
    ë°©ë²•2: ê¸°ì¤€ë³„ 1í”½ (5ê°œ ì„¸íŠ¸)
    ê° ê¸°ì¤€ì—ì„œ ìµœê³ ì  1ê°œì”© ì„ ë³„.

    Returns:
        list[dict]: [{"criterion": ..., "item": ...}, ...]
    """
    now_ts = time.time()
    max_points = max((item["points"] for item in all_items), default=1) or 1
    max_comments = max((item["comments"] for item in all_items), default=1) or 1

    criteria = {
        "ìµœì‹ ì„±": lambda item: 25 - min((now_ts - item["timestamp"]) / 3600, 48) * 0.52 if item["timestamp"] > 0 else 0,
        "ì†ŒìŠ¤ ë‹¤ì–‘ì„±": lambda item: sum(
            1 for other in all_items
            if other["source"] != item["source"] and _title_similarity(item["title"], other["title"]) > 0.3
        ),
        "ë°˜ì‘/ì¸ê¸°": lambda item: item["points"],
        "AI ê´€ë ¨ì„±": lambda item: len(AI_KEYWORDS.findall(item["title"])) * 10 + (5 if AI_KEYWORDS.search(item.get("url", "")) else 0),
        "í† ë¡ ì„±": lambda item: item["comments"],
    }

    picks = []
    used_titles = set()

    for criterion_name, score_fn in criteria.items():
        best = None
        best_score = -1
        for item in all_items:
            # ì´ë¯¸ ì„ íƒëœ ê²ƒê³¼ ì¤‘ë³µ ë°©ì§€
            is_dup = any(_title_similarity(item["title"], t) > 0.4 for t in used_titles)
            if is_dup:
                continue
            s = score_fn(item)
            if s > best_score:
                best_score = s
                best = item
        if best:
            picks.append({"criterion": criterion_name, "item": best})
            used_titles.add(best["title"])

    return picks


def select_thread_worthy(all_items, top_n=5):
    """
    ìŠ¤ë ˆë“œ ì¸ê¸°ê¸€ íŒ¨í„´ ë¶„ì„ ê¸°ë°˜ ê¸°ì‚¬ ì„ ë³„.

    íŒ¨í„´ (ìš°ì„ ìˆœìœ„):
    1. ì „ë¬¸ì§€ì‹/ê²½í—˜ ê³µìœ í˜• â€” ê¸°ìˆ  ê¹Šì´ê°€ ìˆëŠ” ê¸€
    2. í† ë¡ /ë…¼ìŸ ìœ ë„í˜• â€” ëŒ“ê¸€ ë§ì€ ê¸€
    3. ì¶©ê²© ìˆ«ì/ë°˜ì „í˜• â€” êµ¬ì²´ì  ìˆ«ìê°€ ìˆëŠ” ê¸€
    4. íŠ¸ë Œë“œ ì†ë³´í˜• â€” ìµœì‹  ë‰´ìŠ¤
    5. ê³µê°í˜• â€” ì§ì¥ì¸/ê°œë°œì ê°ì • í„°ì¹˜

    Returns:
        list[dict]: [{"pattern": ..., "item": ..., "reason": ...}, ...]
    """
    now_ts = time.time()
    # ìˆ«ìê°€ í¬í•¨ëœ ì œëª© ê°ì§€
    has_number = re.compile(r'\d+[%$ì–µë§Œì²œ]|\$\d|billion|million')

    # ë…¼ìŸ í‚¤ì›Œë“œ
    debate_words = re.compile(r'(vs|versus|debate|controversy|ban|block|concern|risk|danger|threat|ethical|privacy|layoff|replace|kill|die|end|crisis|scandal|fired)', re.I)

    # ê³µê° í‚¤ì›Œë“œ
    empathy_words = re.compile(r'(developer|engineer|worker|employee|job|career|salary|burnout|remote|work|hire|fired|layoff)', re.I)

    patterns = []

    # 1. ì „ë¬¸ì§€ì‹í˜•: AIê´€ë ¨ + points ë†’ì€ ê²ƒ
    tech_items = [i for i in all_items if AI_KEYWORDS.search(i["title"]) and i["points"] > 0]
    tech_items.sort(key=lambda x: x["points"], reverse=True)

    # 2. í† ë¡ í˜•: ëŒ“ê¸€ ë§ì€ ê²ƒ
    debate_items = sorted(all_items, key=lambda x: x["comments"], reverse=True)

    # 3. ì¶©ê²© ìˆ«ìí˜•: ìˆ«ìê°€ ìˆëŠ” ì œëª©
    number_items = [i for i in all_items if has_number.search(i["title"])]
    number_items.sort(key=lambda x: x.get("_score", 0), reverse=True)

    # 4. ì†ë³´í˜•: ê°€ì¥ ìµœì‹ 
    recent_items = [i for i in all_items if i["timestamp"] > 0]
    recent_items.sort(key=lambda x: x["timestamp"], reverse=True)

    # 5. ê³µê°í˜•: ì§ì¥/ì»¤ë¦¬ì–´ ê´€ë ¨
    empathy_items = [i for i in all_items if empathy_words.search(i["title"])]
    empathy_items.sort(key=lambda x: x.get("_score", 0), reverse=True)

    used_titles = set()

    def pick_first_unused(candidates, pattern_name, reason):
        for item in candidates:
            is_dup = any(_title_similarity(item["title"], t) > 0.4 for t in used_titles)
            if not is_dup:
                used_titles.add(item["title"])
                patterns.append({"pattern": pattern_name, "item": item, "reason": reason})
                return True
        return False

    pick_first_unused(tech_items, "ì „ë¬¸ì§€ì‹ ê³µìœ í˜•", "ê¸°ìˆ  ê¹Šì´ + ë†’ì€ ê´€ì‹¬")
    pick_first_unused(debate_items, "í† ë¡ /ë…¼ìŸ ìœ ë„í˜•", "ëŒ“ê¸€ í­ë°œ â†’ ì•Œê³ ë¦¬ì¦˜ ë¶€ìŠ¤íŠ¸")
    pick_first_unused(number_items, "ì¶©ê²© ìˆ«ìí˜•", "êµ¬ì²´ì  ìˆ«ì â†’ ì²´ë¥˜ì‹œê°„ ì¦ê°€")
    pick_first_unused(recent_items, "íŠ¸ë Œë“œ ì†ë³´í˜•", "24ì‹œê°„ ë‚´ ë¹ ë¥¸ ë°˜ì‘")
    pick_first_unused(empathy_items, "ê³µê°í˜•", "ì§ì¥ì¸/ê°œë°œì ê°ì • í„°ì¹˜")

    # 5ê°œ ë¯¸ë§Œì´ë©´ ë‚˜ë¨¸ì§€ë¥¼ ìŠ¤ì½”ì–´ ê¸°ì¤€ìœ¼ë¡œ ì±„ì›€
    if len(patterns) < top_n:
        remaining = sorted(all_items, key=lambda x: x.get("_score", 0), reverse=True)
        for item in remaining:
            if len(patterns) >= top_n:
                break
            is_dup = any(_title_similarity(item["title"], t) > 0.4 for t in used_titles)
            if not is_dup:
                used_titles.add(item["title"])
                patterns.append({"pattern": "ì¶”ê°€ ì„ ë³„", "item": item, "reason": "ì¢…í•© ìŠ¤ì½”ì–´ ìƒìœ„"})

    return patterns[:top_n]


def _scrape_summary(url, max_sentences=2):
    """ê¸°ì‚¬ URLì—ì„œ í•µì‹¬ ìš”ì•½ 1-2ë¬¸ì¥ ì¶”ì¶œ"""
    text = scrape_article(url, timeout=8)
    if not text or len(text) < 50:
        return ""
    sentences = re.split(r'(?<=[.!?])\s+', text)
    key = [s.strip() for s in sentences if 20 < len(s.strip()) < 200][:max_sentences]
    if not key:
        return ""
    summary = " ".join(key)
    if len(summary) > 200:
        summary = summary[:197] + "..."
    return summary


def format_top_news(top_items, label="ë°©ë²•1: ê°€ì¤‘ì¹˜ í†µí•© ë­í‚¹", with_scrape=True):
    """ì¢…í•© ë‰´ìŠ¤ë¥¼ í…”ë ˆê·¸ë¨ í¬ë§·ìœ¼ë¡œ ë³€í™˜. ê¸°ì‚¬ ìš”ì•½ í¬í•¨."""
    if not top_items:
        return f"{label}ì„ ìƒì„±í•˜ì§€ ëª»í–ˆì–´ìš”."

    lines = [f"ğŸ“Œ {label}", ""]
    for i, item in enumerate(top_items, 1):
        src = item.get("source", "")
        score = item.get("_score", 0)
        badge = ""
        if AI_KEYWORDS.search(item["title"]):
            badge = " [AI]"
        lines.append(f"{i}. {item['title']}{badge} ({score}ì )")
        lines.append(f"   [{src}] {item['url']}")
        if with_scrape:
            summary = _scrape_summary(item["url"])
            if summary:
                lines.append(f"   â†’ {summary}")
        lines.append("")

    return "\n".join(lines).strip()


def format_criterion_picks(picks, with_scrape=True):
    """ê¸°ì¤€ë³„ 1í”½ì„ í…”ë ˆê·¸ë¨ í¬ë§·ìœ¼ë¡œ ë³€í™˜. ê¸°ì‚¬ ìš”ì•½ í¬í•¨."""
    if not picks:
        return "ê¸°ì¤€ë³„ 1í”½ì„ ìƒì„±í•˜ì§€ ëª»í–ˆì–´ìš”."

    lines = ["ğŸ“Œ ë°©ë²•2: ê¸°ì¤€ë³„ 1í”½ (5ê°œ ì„¸íŠ¸)", ""]
    for p in picks:
        item = p["item"]
        lines.append(f"{p['criterion']} â†’ {item['title']}")
        lines.append(f"   [{item.get('source', '')}] {item['url']}")
        if with_scrape:
            summary = _scrape_summary(item["url"])
            if summary:
                lines.append(f"   â†’ {summary}")
        lines.append("")

    return "\n".join(lines).strip()


def format_thread_picks(thread_picks):
    """ìŠ¤ë ˆë“œ ë§ì¶¤ ê¸°ì‚¬ë¥¼ í…”ë ˆê·¸ë¨ í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
    if not thread_picks:
        return "ìŠ¤ë ˆë“œ ë§ì¶¤ ê¸°ì‚¬ë¥¼ ì„ ë³„í•˜ì§€ ëª»í–ˆì–´ìš”."

    lines = ["ğŸ¯ ìŠ¤ë ˆë“œ ë§ì¶¤ ê¸°ì‚¬ (íŒ¨í„´ ë¶„ì„ ê¸°ë°˜)", ""]
    for i, p in enumerate(thread_picks, 1):
        item = p["item"]
        lines.append(f"{i}. {item['title']}")
        lines.append(f"   â†’ {p['pattern']}. {p['reason']}")
        lines.append(f"   {item['url']}")
        lines.append("")

    return "\n".join(lines).strip()


def scrape_article(url, timeout=10):
    """
    ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ.

    ì†ŒìŠ¤ë³„ íŠ¹í™” íŒŒì‹± + ë²”ìš© fallback.
    ë„¤ë¹„ê²Œì´ì…˜, ê´‘ê³ , ì¿ í‚¤ ì•Œë¦¼ ë“± ë…¸ì´ì¦ˆ ì œê±°.

    Returns:
        str: ê¹¨ë—í•œ ê¸°ì‚¬ ë³¸ë¬¸ (ìµœëŒ€ 2000ì) ë˜ëŠ” ë¹ˆ ë¬¸ìì—´
    """
    try:
        req = Request(url, headers={"User-Agent": _UA})
        with urlopen(req, timeout=timeout) as resp:
            html = resp.read().decode("utf-8", errors="replace")

        # ë…¸ì´ì¦ˆ íƒœê·¸ ì œê±°
        for tag in ['script', 'style', 'nav', 'header', 'footer', 'aside',
                     'noscript', 'iframe', 'form', 'svg', 'button']:
            html = re.sub(rf'<{tag}[^>]*>.*?</{tag}>', '', html, flags=re.DOTALL | re.I)

        # HTML ì£¼ì„ ì œê±°
        html = re.sub(r'<!--.*?-->', '', html, flags=re.DOTALL)

        # ì†ŒìŠ¤ë³„ íŠ¹í™” íŒŒì‹±
        text = ""

        if "news.hada.io" in url:
            # GeekNews: topictextbox í´ë˜ìŠ¤ ì•ˆì˜ ë³¸ë¬¸
            match = re.search(r'class="topictextbox"[^>]*>(.*?)</div>', html, re.DOTALL | re.I)
            if match:
                text = _clean_html_text(match.group(1))

        elif "techcrunch.com" in url:
            # TechCrunch: article íƒœê·¸ ì•ˆì˜ p íƒœê·¸
            match = re.search(r'<article[^>]*>(.*?)</article>', html, re.DOTALL | re.I)
            if match:
                text = _extract_paragraphs(match.group(1))

        elif "technologyreview.com" in url:
            # MIT Tech Review: article body
            match = re.search(r'<article[^>]*>(.*?)</article>', html, re.DOTALL | re.I)
            if match:
                text = _extract_paragraphs(match.group(1))

        elif "news.ycombinator.com" in url:
            # HN í† ë¡  í˜ì´ì§€: ì œëª©ë§Œ (ë³¸ë¬¸ì€ ì™¸ë¶€ ë§í¬)
            match = re.search(r'class="titleline"[^>]*>.*?<a[^>]*>(.*?)</a>', html, re.DOTALL | re.I)
            if match:
                text = _clean_html_text(match.group(1))

        # ë²”ìš© fallback: <article> â†’ <main> â†’ <p> íƒœê·¸
        if not text or len(text) < 100:
            for container_tag in ['article', 'main', r'div[^>]*class="[^"]*(?:content|post|entry|story|body)[^"]*"']:
                match = re.search(rf'<{container_tag}[^>]*>(.*?)</{container_tag.split("[")[0]}>', html, re.DOTALL | re.I)
                if match:
                    extracted = _extract_paragraphs(match.group(1))
                    if len(extracted) > len(text):
                        text = extracted

        # ìµœí›„ fallback: ì „ì²´ <p> íƒœê·¸
        if not text or len(text) < 100:
            text = _extract_paragraphs(html)

        return text[:2000]

    except Exception as e:
        print(f"[WARN] ê¸°ì‚¬ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ({url}): {e}")
        return ""


def _clean_html_text(html_fragment):
    """HTML ì¡°ê°ì—ì„œ íƒœê·¸ ì œê±° í›„ ê¹¨ë—í•œ í…ìŠ¤íŠ¸ ë°˜í™˜"""
    text = re.sub(r'<[^>]+>', ' ', html_fragment)
    text = re.sub(r'&[a-z]+;', ' ', text)  # HTML entities
    text = re.sub(r'&#\d+;', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text


# ë…¸ì´ì¦ˆ í…ìŠ¤íŠ¸ íŒ¨í„´ (ë„¤ë¹„ê²Œì´ì…˜, ê´‘ê³ , ì¿ í‚¤ ë“±)
_NOISE_PATTERNS = re.compile(
    r'(?:cookie|subscribe|newsletter|sign up|log in|logged in|loading|'
    r'advertisement|sponsored|share this|follow us|read more|click here|'
    r'terms of|privacy policy|copyright|all rights reserved|'
    r'ëŒ“ê¸€|êµ¬ë…|ë¡œê·¸ì¸|íšŒì›ê°€ì…|ê´‘ê³ |ê³µìœ í•˜ê¸°|ì´ì „ ê¸°ì‚¬|ë‹¤ìŒ ê¸°ì‚¬)',
    re.I
)


def _extract_paragraphs(html_fragment):
    """HTML ì¡°ê°ì—ì„œ <p> íƒœê·¸ ë³¸ë¬¸ë§Œ ì¶”ì¶œ. ë…¸ì´ì¦ˆ í•„í„°ë§ í¬í•¨."""
    paragraphs = re.findall(r'<p[^>]*>(.*?)</p>', html_fragment, re.DOTALL | re.I)
    text_parts = []
    for p in paragraphs:
        clean = re.sub(r'<[^>]+>', '', p).strip()
        clean = re.sub(r'&[a-z]+;', ' ', clean)
        clean = re.sub(r'&#\d+;', ' ', clean)
        clean = re.sub(r'\s+', ' ', clean).strip()
        # ë…¸ì´ì¦ˆ í•„í„°ë§
        if len(clean) < 30:
            continue
        if _NOISE_PATTERNS.search(clean):
            continue
        text_parts.append(clean)
    return "\n\n".join(text_parts)


def generate_thread_drafts(thread_picks):
    """
    ìŠ¤ë ˆë“œ ë§ì¶¤ ê¸°ì‚¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ context.md ê³µì‹ì— ë”°ë¼ ì™„ì„±í˜• ê¸€ ì´ˆì•ˆ ìƒì„±.

    3ë‹¨ êµ¬ì¡°:
    1. í›… â€” ì²« ì¤„ì—ì„œ ìŠ¤í¬ë¡¤ì„ ë©ˆì¶”ê²Œ
    2. ë³¸ë¬¸ â€” ê³µê° + êµ¬ì²´ì  ì‚¬ì‹¤ + ë°˜ì „
    3. ì°¸ì—¬ ìœ ë„ â€” ëŒ“ê¸€ì„ ì“°ê²Œ ë§Œë“œëŠ” ë§ˆë¬´ë¦¬

    Returns:
        list[dict]: [{"title": ..., "draft": ..., "url": ..., "pattern": ...}, ...]
    """
    drafts = []

    # í•´ì‹œíƒœê·¸ ë§¤í•‘
    tag_map = {
        "ai": "#AI", "openai": "#OpenAI", "gpt": "#GPT", "chatgpt": "#ChatGPT",
        "claude": "#Claude", "anthropic": "#Anthropic", "gemini": "#Gemini",
        "llm": "#LLM", "apple": "#Apple", "google": "#Google",
        "microsoft": "#Microsoft", "meta": "#Meta", "nvidia": "#NVIDIA",
        "copilot": "#Copilot", "mistral": "#Mistral", "llama": "#Llama",
    }

    for i, pick in enumerate(thread_picks):
        item = pick["item"]
        pattern = pick.get("pattern", "ì¶”ê°€ ì„ ë³„")
        title = item["title"]
        url = item["url"]

        # ê¸°ì‚¬ ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘
        print(f"[INFO] ê¸°ì‚¬ ìŠ¤í¬ë˜í•‘ ì¤‘: {title[:50]}...")
        article_text = scrape_article(url)

        # í•´ì‹œíƒœê·¸ ì¶”ì¶œ (1ê°œë§Œ)
        title_lower = title.lower()
        tag = "#AI"  # ê¸°ë³¸
        for keyword, t in tag_map.items():
            if keyword in title_lower:
                tag = t
                break

        # ê¸°ì‚¬ì—ì„œ í•µì‹¬ ìš”ì†Œ ì¶”ì¶œ
        facts = _extract_key_facts(article_text, title)

        # context.md 3ë‹¨ êµ¬ì¡° ê¸°ë°˜ ì´ˆì•ˆ ìƒì„±
        draft = _build_draft_from_pattern(pattern, title, facts, tag)

        drafts.append({
            "title": title,
            "draft": draft,
            "url": url,
            "source": item.get("source", ""),
            "pattern": pattern,
            "reason": pick.get("reason", ""),
            "article_summary": facts.get("summary", "")[:300],
        })

    return drafts


def _extract_key_facts(article_text, title):
    """
    ê¸°ì‚¬ ë³¸ë¬¸ì—ì„œ ì´ˆì•ˆ ì‘ì„±ì— í•„ìš”í•œ í•µì‹¬ ìš”ì†Œ ì¶”ì¶œ.

    Returns:
        dict: {summary, numbers, key_sentences, entities, has_controversy}
    """
    facts = {
        "summary": "",
        "numbers": [],
        "key_sentences": [],
        "entities": [],
        "has_controversy": False,
    }

    if not article_text:
        return facts

    # ìˆ«ì/ë°ì´í„° ì¶”ì¶œ
    number_pattern = re.compile(
        r'(?:\$[\d,.]+\s?(?:billion|million|trillion|B|M|T)?|'
        r'[\d,.]+\s?(?:billion|million|trillion|percent|%|ë‹¬ëŸ¬|ì–µ|ë§Œ|ì²œ|ì¡°|ëª…|ê±´|ê°œ|ë°°))',
        re.I
    )
    facts["numbers"] = list(set(number_pattern.findall(title + " " + article_text)))[:5]

    # ë¬¸ì¥ ë¶„ë¦¬ í›„ í•µì‹¬ ë¬¸ì¥ ì„ ë³„
    sentences = re.split(r'(?<=[.!?])\s+', article_text)
    scored_sentences = []
    for s in sentences:
        s = s.strip()
        if len(s) < 20 or len(s) > 300:
            continue
        score = 0
        # ìˆ«ìê°€ ìˆìœ¼ë©´ ê°€ì‚°
        if number_pattern.search(s):
            score += 3
        # ì¸ëª…/íšŒì‚¬ëª… ìˆìœ¼ë©´ ê°€ì‚°
        if re.search(r'[A-Z][a-z]+(?:\s[A-Z][a-z]+)+', s):
            score += 1
        # ì§§ì€ ë¬¸ì¥ ì„ í˜¸
        if len(s) < 100:
            score += 1
        scored_sentences.append((score, s))

    scored_sentences.sort(key=lambda x: x[0], reverse=True)
    facts["key_sentences"] = [s for _, s in scored_sentences[:5]]

    # ìš”ì•½ (ìƒìœ„ 3ë¬¸ì¥)
    if facts["key_sentences"]:
        facts["summary"] = " ".join(facts["key_sentences"][:3])

    # ë…¼ìŸì„± ì²´í¬
    controversy_words = re.compile(r'(controversy|debate|ban|block|concern|risk|danger|threat|privacy|layoff|replace|fired|criticism|backlash|oppose)', re.I)
    facts["has_controversy"] = bool(controversy_words.search(article_text))

    return facts


def _build_draft_from_pattern(pattern, title, facts, tag):
    """
    íŒ¨í„´ + ê¸°ì‚¬ íŒ©íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ 3ë‹¨ êµ¬ì¡° ì´ˆì•ˆ ìƒì„±.

    context.mdì˜ ê³µì‹:
    - í›…: ìˆ«ì/ì¶©ê²© ì‚¬ì‹¤/ê²°ë¡  ìˆ¨ê¸°ê¸°ë¡œ ì‹œì‘
    - ë³¸ë¬¸: ì§§ì€ ë¬¸ë‹¨, 2-3ì¤„ì”©, ì „í™˜ì–´ ì‚¬ìš©
    - ê°œì¸ ì˜ê²¬: í•œ ì¤„
    - ë§ˆë¬´ë¦¬: ì—´ë¦° ì§ˆë¬¸ or ì–‘ìíƒì¼
    """
    numbers = facts.get("numbers", [])
    key_sentences = facts.get("key_sentences", [])
    has_summary = bool(key_sentences)

    # ë³¸ë¬¸ì— ì‚¬ìš©í•  íŒ©íŠ¸ ë¬¸ì¥ë“¤ (ê°€ë…ì„±ì„ ìœ„í•´ ì§§ê²Œ ì •ë¦¬)
    fact_lines = []
    for s in key_sentences[:3]:
        # ì˜ì–´ ë¬¸ì¥ì€ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ë²ˆì—­ì€ PMì´ ì»¨íŒ ì‹œ ìˆ˜ì •)
        if len(s) > 150:
            s = s[:147] + "..."
        fact_lines.append(s)

    if pattern == "ì „ë¬¸ì§€ì‹ ê³µìœ í˜•":
        hook = f"ì´ê±° ì¢€ í¥ë¯¸ë¡œìš´ ì–˜ê¸´ë°"
        if numbers:
            body_intro = f"\n{title}\n"
        else:
            body_intro = f"\n{title}\n"
        body_facts = "\n\n".join(fact_lines) if fact_lines else "(ê¸°ì‚¬ í•µì‹¬ ë‚´ìš© 2-3ì¤„)"
        opinion = "ì´ìª½ìœ¼ë¡œ ì¢€ íŒŒë´¤ëŠ”ë° ìƒê°ë³´ë‹¤ ê¹ŠìŒ"
        cta = "ê¶ê¸ˆí•œ ê±° ìˆìœ¼ë©´ ë¬¼ì–´ë´ ğŸ’¬"

    elif pattern == "í† ë¡ /ë…¼ìŸ ìœ ë„í˜•":
        hook = f"ì†”ì§íˆ ì´ê±° ë…¼ë€ì´ ë  ë§Œí•œë°"
        body_intro = f"\n{title}\n"
        body_facts = "\n\n".join(fact_lines) if fact_lines else "(ì–‘ìª½ ì‹œê° ì •ë¦¬ 2-3ì¤„)"
        opinion = "ì†”ì§íˆ ì´ê±´ ì˜ê²¬ ì¢€ ê°ˆë¦´ ë“¯"
        cta = "ì´ê±´ ê·œì œê°€ ë‹µì´ì•¼ ê¸°ìˆ ë¡œ ë§‰ì„ ìˆ˜ ìˆëŠ” ë¬¸ì œì•¼?"

    elif pattern == "ì¶©ê²© ìˆ«ìí˜•":
        if numbers:
            hook = f"ìˆ«ìë§Œ ë³´ë©´ ì§„ì§œ ë¯¸ì³¤ë‹¤"
        else:
            hook = f"ì´ê±´ ì¢€ ì¶©ê²©ì¸ë°"
        body_intro = f"\n{title}\n"
        body_facts = "\n\n".join(fact_lines) if fact_lines else "(êµ¬ì²´ì  ìˆ«ì/ë°ì´í„° ê°•ì¡° 2-3ì¤„)"
        opinion = "ê·¼ë° ì§„ì§œ í¬ì¸íŠ¸ëŠ” ë”°ë¡œ ìˆìŒ"
        cta = "ì´ê²Œ ì§„ì§œ í˜„ì‹¤ì´ ë  ê²ƒ ê°™ì•„?"

    elif pattern == "íŠ¸ë Œë“œ ì†ë³´í˜•":
        hook = f"ë°©ê¸ˆ í„°ì§„ ë‰´ìŠ¤ì¸ë°"
        body_intro = f"\n{title}\n"
        body_facts = "\n\n".join(fact_lines) if fact_lines else "(ë¹ ë¥¸ ìš”ì•½ + ì™œ ì¤‘ìš”í•œì§€ 2-3ì¤„)"
        opinion = "ì´ê±° íŒŒê¸‰ë ¥ í´ ê²ƒ ê°™ì€ë°"
        cta = "ì•ìœ¼ë¡œ ì–´ë–»ê²Œ ë ì§€ ì§€ì¼œë´ì•¼ê² ë‹¤"

    elif pattern == "ê³µê°í˜•":
        hook = f"ì†”ì§íˆ ì¢€ ë¶ˆì•ˆí•œ ì–˜ê¸´ë°"
        body_intro = f"\n{title}\n"
        body_facts = "\n\n".join(fact_lines) if fact_lines else "(ê³µê° í¬ì¸íŠ¸ + ë¶ˆì•ˆ ìš”ì†Œ 2-3ì¤„)"
        opinion = "ë‚˜ë„ ìš”ì¦˜ ì´ê±° ë•Œë¬¸ì— ì¢€ ìƒê°ì´ ë§ì•„"
        cta = "ë‹¤ë“¤ ë¹„ìŠ·í•œ ê²½í—˜ ìˆì„ ê²ƒ ê°™ì€ë°"

    else:
        hook = f"ì´ê±° í•œë²ˆ ë´ë´"
        body_intro = f"\n{title}\n"
        body_facts = "\n\n".join(fact_lines) if fact_lines else "(ê¸°ì‚¬ í•µì‹¬ ë‚´ìš© 2-3ì¤„)"
        opinion = "ê°œì¸ì ìœ¼ë¡œ ì¢€ í¥ë¯¸ë¡œì›€"
        cta = "ì–´ë–»ê²Œ ìƒê°í•´?"

    # 3ë‹¨ êµ¬ì¡°ë¡œ ì¡°ë¦½
    draft = f"""{hook}

{body_intro}
{body_facts}

{opinion}

{cta}

{tag}"""

    # ì—°ì† ë¹ˆ ì¤„ ì •ë¦¬
    draft = re.sub(r'\n{3,}', '\n\n', draft).strip()
    return draft


def format_thread_drafts(drafts):
    """ìŠ¤ë ˆë“œ ê¸€ ì´ˆì•ˆì„ í…”ë ˆê·¸ë¨ í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
    if not drafts:
        return "ìŠ¤ë ˆë“œ ê¸€ ì´ˆì•ˆì„ ìƒì„±í•˜ì§€ ëª»í–ˆì–´ìš”."

    lines = ["**ìŠ¤ë ˆë“œ ê¸€ ì´ˆì•ˆ 5ê°œ**", "(ì»¨íŒí•˜ë©´ ê²Œì‹œí• ê²Œìš”!)", ""]
    for i, d in enumerate(drafts, 1):
        lines.append(f"--- ì´ˆì•ˆ {i} ---")
        lines.append(d["draft"])
        lines.append("")

    return "\n".join(lines).strip()


def save_thread_drafts(drafts):
    """
    ìŠ¤ë ˆë“œ ê¸€ ì´ˆì•ˆì„ tasks/briefing_{ë‚ ì§œ}/threads_drafts.mdì— ì €ì¥.

    Returns:
        str: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ (ë˜ëŠ” ë¹ˆ ë¬¸ìì—´)
    """
    if not drafts:
        return ""

    today = datetime.now().strftime("%Y%m%d")
    dir_path = os.path.join(PROJECT_ROOT, "tasks", f"briefing_{today}")
    os.makedirs(dir_path, exist_ok=True)

    file_path = os.path.join(dir_path, "threads_drafts.md")

    lines = [
        f"# ìŠ¤ë ˆë“œ ê¸€ ì´ˆì•ˆ - {datetime.now().strftime('%Y-%m-%d %H:%M')}",
        "",
        "ì•„ë˜ ì´ˆì•ˆ ì¤‘ ê²Œì‹œí•  ê¸€ì„ ì„ íƒí•´ì£¼ì„¸ìš”.",
        "ì»¨íŒí•˜ë©´ threads_poster.pyë¡œ ìë™ ê²Œì‹œí•©ë‹ˆë‹¤.",
        "",
    ]

    for i, d in enumerate(drafts, 1):
        lines.append(f"## ì´ˆì•ˆ {i} [{d['source']}]")
        lines.append("")
        lines.append(f"- íŒ¨í„´: {d.get('pattern', '')}")
        lines.append(f"- URL: {d.get('url', '')}")
        lines.append(f"ìƒíƒœ: ëŒ€ê¸°")
        lines.append("")
        lines.append("### ê¸€ ì´ˆì•ˆ")
        lines.append("```")
        lines.append(d["draft"])
        lines.append("```")
        lines.append("")
        # ê¸°ì‚¬ ìš”ì•½ (PMì´ ê¸€ ì™„ì„± ì‹œ ì°¸ê³ )
        summary = d.get("article_summary", "")
        if summary:
            lines.append("### ê¸°ì‚¬ ìš”ì•½ (ì°¸ê³ ìš©)")
            lines.append(f"> {summary}")
            lines.append("")

    try:
        with open(file_path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        print(f"[OK] ìŠ¤ë ˆë“œ ì´ˆì•ˆ ì €ì¥: {file_path}")
        return file_path
    except Exception as e:
        print(f"[WARN] ìŠ¤ë ˆë“œ ì´ˆì•ˆ ì €ì¥ ì‹¤íŒ¨: {e}")
        return ""


def generate_briefing():
    """ì¼ì¼ ë¸Œë¦¬í•‘ ìƒì„±"""
    now = datetime.now()
    briefing_parts = []

    briefing_parts.append(f"**heysquid ì¼ì¼ ë¸Œë¦¬í•‘**")
    briefing_parts.append(f"{now.strftime('%Y-%m-%d %A')}")
    briefing_parts.append("")

    # ë¯¸ì²˜ë¦¬ ë©”ì‹œì§€
    pending = get_pending_tasks()
    if pending > 0:
        briefing_parts.append(f"**ë¯¸ì²˜ë¦¬ ë©”ì‹œì§€: {pending}ê°œ**")
    else:
        briefing_parts.append("ë¯¸ì²˜ë¦¬ ë©”ì‹œì§€: ì—†ìŒ")
    briefing_parts.append("")

    # ì›Œí¬ìŠ¤í˜ì´ìŠ¤ë³„ ìƒíƒœ
    workspaces_file = os.path.join(PROJECT_ROOT, "data", "workspaces.json")

    if os.path.exists(workspaces_file):
        try:
            with open(workspaces_file, "r", encoding="utf-8") as f:
                workspaces = json.load(f)
        except Exception:
            workspaces = {}
    else:
        workspaces = {}

    if workspaces:
        briefing_parts.append("**í”„ë¡œì íŠ¸ í˜„í™©:**")
        briefing_parts.append("")

        for name, info in workspaces.items():
            ws_path = info.get("path", "")
            description = info.get("description", "")
            last_active = info.get("last_active", "N/A")

            briefing_parts.append(f"--- {name} ---")
            briefing_parts.append(f"  {description}")
            briefing_parts.append(f"  ìµœê·¼ í™œë™: {last_active}")

            # Git ìš”ì•½
            if os.path.exists(ws_path):
                git_summary = get_git_summary(ws_path)
                briefing_parts.append(f"  {git_summary}")

            # ì§„í–‰ ê¸°ë¡
            progress = get_recent_progress(name)
            if progress != "ì§„í–‰ ê¸°ë¡ ì—†ìŒ":
                briefing_parts.append(f"  ìµœê·¼ ì§„í–‰:")
                briefing_parts.append(f"  {progress}")

            briefing_parts.append("")

    else:
        briefing_parts.append("ë“±ë¡ëœ í”„ë¡œì íŠ¸ ì—†ìŒ")
        briefing_parts.append("")

    # ë©€í‹°ì†ŒìŠ¤ ë‰´ìŠ¤ ìˆ˜ì§‘ + ë¶„ì„
    try:
        all_news = fetch_all_news_sources()

        # ë°©ë²•1: ê°€ì¤‘ì¹˜ í†µí•© ë­í‚¹ TOP 10
        top10 = score_and_rank_news(all_news, top_n=10)
        if top10:
            briefing_parts.append(format_top_news(top10, "ë°©ë²•1: ê°€ì¤‘ì¹˜ í†µí•© ë­í‚¹ TOP 10"))
            briefing_parts.append("")

        # ë°©ë²•2: ê¸°ì¤€ë³„ 1í”½ (5ê°œ ì„¸íŠ¸)
        criterion_picks = pick_best_per_criterion(all_news)
        if criterion_picks:
            briefing_parts.append(format_criterion_picks(criterion_picks))
            briefing_parts.append("")

        # ìŠ¤ë ˆë“œ ë§ì¶¤ ê¸°ì‚¬ ì„ ë³„ (ì¸ê¸°ê¸€ íŒ¨í„´ ê¸°ë°˜)
        thread_picks = select_thread_worthy(all_news, top_n=5)
        if thread_picks:
            briefing_parts.append(format_thread_picks(thread_picks))
            briefing_parts.append("")

            # ìŠ¤ë ˆë“œ ê¸€ ì´ˆì•ˆ (ìì—°ìŠ¤ëŸ¬ìš´ í†¤)
            drafts = generate_thread_drafts(thread_picks)
            if drafts:
                briefing_parts.append(format_thread_drafts(drafts))
                briefing_parts.append("")
                save_thread_drafts(drafts)

    except Exception as e:
        print(f"[WARN] ì¢…í•© ë‰´ìŠ¤/ìŠ¤ë ˆë“œ ìƒì„± ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        briefing_parts.append("(ì¢…í•© ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ)")
        briefing_parts.append("")

    # GeekNews TOP 10 (ë³„ë„ ì„¹ì…˜)
    news_items = fetch_geeknews_top(10)
    if news_items:
        briefing_parts.append(format_geeknews(news_items))
        briefing_parts.append("")

    briefing_parts.append("---")
    briefing_parts.append("_heysquid ìë™ ë¸Œë¦¬í•‘_")

    return "\n".join(briefing_parts)


def send_briefing():
    """ë¸Œë¦¬í•‘ ìƒì„± í›„ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡"""
    if not CHAT_ID:
        print("[ERROR] TELEGRAM_ALLOWED_USERSê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False

    briefing = generate_briefing()
    print(briefing)
    print()

    try:
        from .telegram_sender import send_message_sync
        success = send_message_sync(int(CHAT_ID), briefing)

        if success:
            print("[OK] ë¸Œë¦¬í•‘ ì „ì†¡ ì™„ë£Œ!")
        else:
            print("[ERROR] ë¸Œë¦¬í•‘ ì „ì†¡ ì‹¤íŒ¨!")

        return success

    except Exception as e:
        print(f"[ERROR] ë¸Œë¦¬í•‘ ì „ì†¡ ì˜¤ë¥˜: {e}")
        return False


if __name__ == "__main__":
    print("=" * 60)
    print("heysquid ì¼ì¼ ë¸Œë¦¬í•‘")
    print("=" * 60)
    print()

    send_briefing()
